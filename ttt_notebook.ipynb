{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ttt_notebook.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPHpLxP/eB2qIU+tBJkb/Q1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangcongcong123/ttt/blob/master/ttt_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8VSG4nzDf-v",
        "colab_type": "text"
      },
      "source": [
        "## TTT: Fine-tuning Transformers with TPUs or GPUs acceleration, written in Tensorflow2.0+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-W64E2MB69K",
        "colab_type": "text"
      },
      "source": [
        "TTT is a package for fine-tuning **T**ransformers with **T**PUs, written in **T**ensorflow2.0+. It is motivated to be completed due to bugs I found tricky to solve when using the [xla library](https://github.com/pytorch/xla) with PyTorch. As a newcomer to the TF world, I am humble to learn more from the community and hence it is open sourced [here]((https://github.com/wangcongcong123/ttt)). \n",
        "\n",
        "This noteboook guides to train transformers using the ttt [library](https://github.com/wangcongcong123/ttt) in two ways:\n",
        "1. Train with code controls to customize your dataset and configure model parameters for fine-tuning\n",
        "2. Run direct-to-go commands to fine-tune a transformer with single sequence-based classification datasets (you can explore the [nlp viewer](https://huggingface.co/nlp/viewer/) to have a sense of what datasets fit to this feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi8bGXqh1ImI",
        "colab_type": "text"
      },
      "source": [
        "## Prepare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTb8Hd-r06cu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "92ffe62d-2a01-4f49-dada-027e4313cef9"
      },
      "source": [
        "!git clone https://github.com/wangcongcong123/ttt.git\n",
        "%cd ttt\n",
        "!pip install -e ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ttt'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 28 (delta 1), reused 28 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n",
            "/content/ttt\n",
            "Obtaining file:///content/ttt\n",
            "Requirement already satisfied: tensorflow==2.3.0 in /usr/local/lib/python3.6/dist-packages (from ttt==0.0.1) (2.3.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from ttt==0.0.1) (0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from ttt==0.0.1) (4.41.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from ttt==0.0.1) (2.4.3)\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 3.4MB/s \n",
            "\u001b[?25hCollecting nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/e3/bcdc59f3434b224040c1047769c47b82705feca2b89ebbc28311e3764782/nlp-0.4.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 11.0MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 25.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (1.31.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (3.12.4)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (1.18.5)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (2.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (2.3.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0->ttt==0.0.1) (2.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->ttt==0.0.1) (0.22.2.post1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->ttt==0.0.1) (3.13)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp->ttt==0.0.1) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp->ttt==0.0.1) (3.0.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from nlp->ttt==0.0.1) (1.0.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp->ttt==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp->ttt==0.0.1) (0.3.2)\n",
            "Collecting pyarrow>=0.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/99/0a605f016121ca314d1469dc9069e4978395bc46fda40f73099d90ad3ba4/pyarrow-1.0.1-cp36-cp36m-manylinux2014_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 236kB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 42.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers->ttt==0.0.1) (2019.12.20)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 37.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers->ttt==0.0.1) (20.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow==2.3.0->ttt==0.0.1) (49.6.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0->ttt==0.0.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0->ttt==0.0.1) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0->ttt==0.0.1) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0->ttt==0.0.1) (1.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0->ttt==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->ttt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp->ttt==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->nlp->ttt==0.0.1) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp->ttt==0.0.1) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp->ttt==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp->ttt==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp->ttt==0.0.1) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->ttt==0.0.1) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers->ttt==0.0.1) (2.4.7)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0->ttt==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0->ttt==0.0.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0->ttt==0.0.1) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0->ttt==0.0.1) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0->ttt==0.0.1) (1.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0->ttt==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0->ttt==0.0.1) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0->ttt==0.0.1) (3.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=454ea73dd966eaae12324ad6409646354eb40d0753fbf26564d937953f1a22fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tensorboardX, pyarrow, xxhash, nlp, tokenizers, sacremoses, sentencepiece, transformers, ttt\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "  Running setup.py develop for ttt\n",
            "Successfully installed nlp-0.4.0 pyarrow-1.0.1 sacremoses-0.0.43 sentencepiece-0.1.91 tensorboardX-2.1 tokenizers-0.8.1rc2 transformers-3.1.0 ttt xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqonr-vj1Nhn",
        "colab_type": "text"
      },
      "source": [
        "## Train on TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PEEy2dG38ox",
        "colab_type": "text"
      },
      "source": [
        "### Make sure in TPU environment\n",
        "\n",
        "* On the main menu, click Runtime >> select **Change runtime type**. Set \"TPU\" as the hardware accelerator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5Y11lju1NBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make sure the right thing is done\n",
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWPlu2Ux2pov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "daaaaf72-3bcb-4cdf-8b0c-336510ab1056"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "TPU_ADDRESS=tpu.cluster_spec().as_dict()['worker'][0].split(\":\")[0]\n",
        "TPU_ADDRESS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.3.0\n",
            "Running on TPU  ['10.86.132.2:8470']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'10.86.132.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkjiQMnb-X1N",
        "colab_type": "text"
      },
      "source": [
        "### Start fine-tuning using T5-small\n",
        "- the following uses SST2 as the dataset\n",
        "- try others: replace `args.data_path=\"data/glue/sst2\"` with `\"data/20newsgroup\"`, `\"data/ag_news\"`, `\"data/imdb\"`, or `\"data/sentiment140\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJg1Ta90Ag97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ttt import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGVh93l8Adde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(args, logger, model_getter):\n",
        "    if args.use_tpu:\n",
        "        # Create distribution strategy\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + args.tpu_address)\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "        logger.info(\"All TPU devices: \")\n",
        "        for each_device in tf.config.list_logical_devices('TPU'):\n",
        "            logger.info(each_device)\n",
        "        strategy = tf.distribute.TPUStrategy(tpu)\n",
        "        # Create model\n",
        "        with strategy.scope():\n",
        "            model = model_getter(args)\n",
        "    else:\n",
        "        if args.use_gpu:\n",
        "            # Create a MirroredStrategy.\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "            logger.info(\"Number of GPU devices: {}\".format(strategy.num_replicas_in_sync))\n",
        "            # Open a strategy scope.\n",
        "            with strategy.scope():\n",
        "                model = model_getter(args)\n",
        "        else:\n",
        "            raise ValueError(\"not available yet\")\n",
        "    logger.info(model.summary())\n",
        "    args.num_replicas_in_sync = strategy.num_replicas_in_sync\n",
        "    write_args(args.output_path, args)\n",
        "    return model, strategy\n",
        "\n",
        "def run():\n",
        "  args = Args()\n",
        "  # check what args are available\n",
        "  logger.info(f\"args: {json.dumps(args.__dict__, indent=2)}\")\n",
        "  ############### customize args\n",
        "  # args.use_gpu = True # if use_gpu, make sure you in a GPU environment first.\n",
        "  args.use_tpu = True\n",
        "  args.do_train = True\n",
        "  args.use_tb = True\n",
        "  # any one from MODELS_SUPPORT (check:ttt/args.py)\n",
        "  args.model_select = \"t5-small\"\n",
        "  # select a dataset. First check if  it is from nlp, if yes load it here and save locally to the data_path\n",
        "  # or customize a data in the data_path (train.json, val.json, test.json) where examples are organised in jsonl format\n",
        "  # each line represents an example like this: {\"text\": \"...\", \"label\",\"...\"}\n",
        "  args.data_path = \"data/glue/sst2\"\n",
        "  # any one from TASKS_SUPPORT (check:ttt/args.py)\n",
        "  args.task = \"t2t\"\n",
        "  args.log_steps = 400\n",
        "  # any one from LR_SCHEDULER_SUPPORT (check:ttt/args.py)\n",
        "  args.scheduler = \"warmuplinear\"\n",
        "  # set do_eval = False if your data does not contain a validation set. In that case, patience, and early_stop will be invalid\n",
        "  args.do_eval = True\n",
        "  args.tpu_address = TPU_ADDRESS\n",
        "  ############### end customize args\n",
        "  # to have a sanity check for the args\n",
        "  sanity_check(args)\n",
        "  # seed everything, make deterministic\n",
        "  set_seed(args.seed)\n",
        "  tokenizer = get_tokenizer(args)\n",
        "  inputs = get_inputs(tokenizer, args)\n",
        "  model, strategy = create_model(args, logger, get_model)\n",
        "  # start training, here we customize T2TTrainer to get more control and flexibility\n",
        "  trainer = T2TTrainer(args)\n",
        "  trainer.train(model, strategy, tokenizer, inputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MWwF2y02hx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEPmix_2BEit",
        "colab_type": "text"
      },
      "source": [
        "-  the fine-tuning takes every 400 steps to evaluate on validation set for model weights selection and saving\n",
        "- this may take a while to be finished. Go grab a cuppa and enjoy the game.\n",
        "- after training, you will find all training details and model weights are saved to `tmp/t5-small_t2t_glue-sst2`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wv5QvC24Grt",
        "colab_type": "text"
      },
      "source": [
        "### Run with commands\n",
        "- remeber to use `--tpu_address` (it is `10.86.132.2` in this case)\n",
        "- this may take a while to be finished. Go grab a cuppa and enjoy the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR9bbzBg3mJt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e0b6e20f-b0e2-4f6b-da13-63c91c792036"
      },
      "source": [
        "!python3 run.py --model_select t5-small --data_path data/glue/sst2 --task t2t --per_device_train_batch_size 8 --num_epochs_train 6 --max_seq_length 128 --lr 5e-5 --schedule warmuplinear --do_train --do_eval --do_test --use_tpu --tpu_address 10.86.132.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "evaluating...:  25% 1/4 [00:05<00:16,  5.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "evaluating...:  50% 2/4 [00:10<00:10,  5.34s/it]\u001b[A\u001b[A\n",
            "\n",
            "evaluating...:  75% 3/4 [00:15<00:05,  5.20s/it]\u001b[A\u001b[A\n",
            "\n",
            "evaluating...: 100% 4/4 [00:17<00:00,  4.48s/it]\n",
            "2020-09-03 21:49:31.793 INFO t2t_trainer - evaluate: \n",
            "\n",
            "2020-09-03 21:49:31.793 INFO t2t_trainer - evaluate: *******eval at global_step = 800 on validation dataset*********\n",
            "2020-09-03 21:49:31.793 INFO t2t_trainer - evaluate: val_acc: 0.8727064220183486\n",
            "2020-09-03 21:49:31.803 INFO t2t_trainer - evaluate: val_cls_report:               precision    recall  f1-score   support\n",
            "\n",
            "    negative     0.8578    0.8879    0.8726       428\n",
            "    positive     0.8881    0.8581    0.8729       444\n",
            "\n",
            "    accuracy                         0.8727       872\n",
            "   macro avg     0.8729    0.8730    0.8727       872\n",
            "weighted avg     0.8732    0.8727    0.8727       872\n",
            "\n",
            "2020-09-03 21:49:31.803 INFO t2t_trainer - evaluate: so far the best check point at global_step=800 based on eval_on acc\n",
            "2020-09-03 21:49:31.826 INFO t2t_trainer - save_ck: save best model weights to tmp/t5-small_t2t_glue-sst2/best_ck_at_global_step_800.h5\n",
            "2020-09-03 21:49:32.900 INFO t2t_trainer - evaluate: best so far(acc): 0.8727064220183486\n",
            "2020-09-03 21:49:32.900 INFO t2t_trainer - evaluate: early stop count: 0/20\n",
            "2020-09-03 21:49:32.901 INFO t2t_trainer - save_ck: save model weights to tmp/t5-small_t2t_glue-sst2/ck_at_global_step_800.h5\n",
            "2020-09-03 21:49:34.134 INFO t2t_trainer - train: train loss at global_step 800: 3.8830751325562596\n",
            "\n",
            "training - epoch 1/6 iter 799: train loss 0.48807. lr 4.860184e-05:  76% 800/1053 [02:44<26:05,  6.19s/it]\u001b[A\n",
            "training - epoch 1/6 iter 800: train loss 0.39267. lr 4.859305e-05:  76% 800/1053 [02:44<26:05,  6.19s/it]\u001b[A\n",
            "training - epoch 1/6 iter 800: train loss 0.39267. lr 4.859305e-05:  76% 801/1053 [02:44<18:19,  4.36s/it]\u001b[ATraceback (most recent call last):\n",
            "  File \"run.py\", line 40, in <module>\n",
            "    trainer.train(model, strategy, tokenizer, inputs)\n",
            "  File \"/content/ttt/ttt/t2t_trainer.py\", line 196, in train\n",
            "    loss = distributed_train_step(x)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 807, in _call\n",
            "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2829, in __call__\n",
            "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\n",
            "    cancellation_manager=cancellation_manager)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1924, in _call_flat\n",
            "    ctx, args, cancellation_manager=cancellation_manager))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 550, in call\n",
            "    ctx=ctx)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
            "    inputs, attrs, num_outputs)\n",
            "KeyboardInterrupt\n",
            "\n",
            "epochs:   0% 0/6 [02:44<?, ?it/s]\n",
            "training - epoch 1/6 iter 800: train loss 0.39267. lr 4.859305e-05:  76% 801/1053 [02:44<00:51,  4.86it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}